<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>PQA: Perceptual Question Answering</title>
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
</head>

<body>

    <br />
    <br />

    <div class="w3-container">
        <div class="w3-content" style="max-width:850px">

            <div class="w3-content w3-center" style="max-width:850px">
                <h2 id="title"><b>PQA: Perceptual Question Answering</b></h2>
                <br />
                <p>
                    <a target="_blank" href="https://qugank.github.io/">Yonggang Qi</a><sup>1*</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a target="_blank">Kai Zhang</a><sup>1*</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a target="_blank">Aneeshan Sain</a><sup>2</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a target="_blank">Yi-Zhe Song</a><sup>2</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                </p>
                <p>
                    <sup>1</sup>Beijing University of Posts and Telecommunications, CN
                    &nbsp; &nbsp; &nbsp;
                    <sup>2</sup>SketchX, CVSSP, University of Surrey, UK
                    &nbsp; &nbsp; &nbsp;
                </p>
            </div>



            <br>
            <div class="w3-content w3-center" style="max-width:850px">
                <img src="front-v2.svg" alt="front" />
                <p>Figure 1. Perceptual Question and Answer (PQA). Given an ex-emplar PQA pair (Left), a new question
                    (right) is required to be addressed, i.e. generate answer-grid from scratch.</p>
            </div>
            <br>
            <h3 class="w3-left-align" id="introduction"><b>Introduction</b></h3>
            <p>
                Perceptual organization remains one of the very few established theories on the human visual system. 
                It underpinned many pre-deep seminal works on segmentation and detection, yet research has seen a rapid decline since the preferential shift to learning deep models. 
                Of the limited attempts, most aimed at interpreting complex visual scenes using perceptual organizational rules. This has however been proven to be sub-optimal, since models were unable to effectively capture the visual complexity in real-world imagery. 
                In this paper, we rejuvenate the study of perceptual organization, by advocating two positional changes: (i) we examine purposefully generated synthetic data, instead of complex real imagery, and (ii) we ask machines to synthesize novel perceptually-valid patterns, instead of explaining existing data. 
                Our overall answer lies with the introduction of a novel visual challenge -- the challenge of perceptual question answering (PQA). 
                Upon observing example perceptual question-answer pairs, the goal for PQA is to solve similar questions by generating answers entirely from scratch (see Figure 1). 
                Our first contribution is therefore the first dataset of perceptual question-answer pairs, each generated specifically for a particular Gestalt principle. 
                We then borrow insights from human psychology to design an agent that casts perceptual organization as a self-attention problem, where a proposed grid-to-grid mapping network directly generates answer patterns from scratch. 
                Experiments show our agent to outperform a selection of naive and strong baselines. 
            </p>


            <h3 class="w3-left-align"><b>Dataset</b></h3>
            <h4 class="w3-left-align"><b>Preview</b></h4>
            <div class="w3-content w3-center" style="max-width:850px">
                <img src="dataset-v2.svg" alt="dataset" style="width:600px" />
                <p>Figure 2. PQA Dataset. Each row from (a) to (g) corresponds to a specific Gestalt law, and a few
                    examples of PQA pair with question (left) and answer (right) are visualized. Zoom in for better
                    visualization.</p>
            </div>
            <h4 class="w3-left-align"><b>Statistic</b></h4>
            <div class="w3-content w3-center" style="max-width:850px">
                <img src="statistic.svg" alt="statistic" style="width:600px" />
                <p>
                    Figure 3.
                    (a) Distribution of key region loca-tions. x and y coordinates are normalized to (0,1),
                    correspondingto
                    the center of key regions.
                    (b) Distribution of grid size. x-axisand y-axis correspond to width and height of a grid.
                </p>
            </div>

            <div class="w3-content w3-center" style="max-width:850px">
                <table class="w3-table w3-bordered w3-border">
                    <tr>
                        <th>Tasks </th>
                        <th>T<sup>1</sup></th>
                        <th>T<sup>2</sup></th>
                        <th>T<sup>3</sup></th>
                        <th>T<sup>4</sup></th>
                        <th>T<sup>5</sup></th>
                        <th>T<sup>6</sup></th>
                        <th>T<sup>7</sup></th>
                    </tr>
                    <tr>
                        <td>Avg Symbols</td>
                        <td>2.0</td>
                        <td>2.0</td>
                        <td>5.0</td>
                        <td>2.0</td>
                        <td>5.0</td>
                        <td>3.0</td>
                        <td>5.0</td>
                    </tr>
                    <tr>
                        <td>Avg Slots (%)</td>
                        <td>12.9</td>
                        <td>3.6</td>
                        <td>4.0</td>
                        <td>7.6</td>
                        <td>15.3</td>
                        <td>9.8</td>
                        <td>12.5</td>
                    </tr>
                </table>
                <p> Table 1. Statistics of PQA dataset. </p>
            </div>
            <p>
                Statistical analysis is provided as shown in Table and Figure above where Avg Symbols indicate the
                number of symbols in a question-grid.
                It basically shows the number of colors that are enough to represent a specified instance of a task.
                For instance, 2 colors in T1 are enough -- one for background and one for the boundary of closure
                region.
                Avg Slots represents the percentage of question-grid needed to be modified to form a correct answer.
            </p>

            <h3 class="w3-left-align" id="publication"><b>Experiment Results</b></h3>

            <div class="w3-content w3-center" style="max-width:850px">
                <img src="result.svg" alt="result" style="width:600px" />
                <p> Figure 4. Testing results on varying training data volume. </p>
            </div>

            <div class="w3-content w3-center" style="max-width:850px">
                <table class="w3-table w3-bordered w3-border">
                    <tr>
                        <th>Method</th>
                        <th>T<sup>1</sup></th>
                        <th>T<sup>2</sup></th>
                        <th>T<sup>3</sup></th>
                        <th>T<sup>4</sup></th>
                        <th>T<sup>5</sup></th>
                        <th>T<sup>6</sup></th>
                        <th>T<sup>7</sup></th>
                        <th>Avg </th>
                    </tr>
                    <tr>
                        <td>ResNet-34</td>
                        <td>79.60</td>
                        <td>17.59</td>
                        <td>17.95</td>
                        <td>85.23</td>
                        <td>0</td>
                        <td>19.55</td>
                        <td>0.06</td>
                        <td>31.43 </td>
                    </tr>
                    <tr>


                        <td>ResNet-101</td>
                        <td>73.93</td>
                        <td>10.60</td>
                        <td>0.08</td>
                        <td>50.89</td>
                        <td>0</td>
                        <td>1.65</td>
                        <td>0</td>
                        <td>19.59 </td>
                    </tr>
                    <tr>
                        <td>LSTM</td>
                        <td>55.70</td>
                        <td>23.16</td>
                        <td>25.61</td>
                        <td>38.24</td>
                        <td>0</td>
                        <td>7.35</td>
                        <td>2.75</td>
                        <td>21.83</td>
                    </tr>
                    <tr>
                        <td>bi-LSTM</td>
                        <td>81.95</td>
                        <td>26.56</td>
                        <td>75.61</td>
                        <td>85.85</td>
                        <td>0</td>
                        <td>41.36</td>
                        <td>23.41</td>
                        <td>47.82</td>
                    </tr>
                    <tr>
                        <td>Transformer</td>
                        <td>16.83</td>
                        <td>11.27</td>
                        <td>87.42</td>
                        <td>0.33</td>
                        <td>0</td>
                        <td>0.08</td>
                        <td>0</td>
                        <td>16.56 </td>
                    </tr>
                    <tr>
                        <td>TD+H-CNN</td>
                        <td> <b> 88.76 </b></td>
                        <td>89.77</td>
                        <td>78.76</td>
                        <td>96.42</td>
                        <td>0</td>
                        <td>50.74</td>
                        <td>9.31</td>
                        <td>59.11</td>
                    </tr>
                    <tr>
                        <td>Ours</td>
                        <td>82.60</td>
                        <td><b>97.64</b></td>
                        <td><b>93.66</b></td>
                        <td><b>96.88</b></td>
                        <td><b>61.83</b></td>
                        <td><b>82.69</b></td>
                        <td><b>98.99</b></td>
                        <td><b>87.76</b></td>
                    </tr>
                </table>
                <p>Table 2. Comparison results (%) of models trained onall tasks.</p>
            </div>

            <p>
                We observe our method which significantly outperforms other competitors over all tasks.
                On inspecting performances of every task further individually in Table2, we realize that T5 is most
                challenging as all baseline methods fail completely.
                On the contrary, it is interesting to note that humans can understand and address the questions in T5
                quite easily.
                Similar trend can be found on T6 and T7 as well. On tasks T1 to T4, all competitors perform better than
                they do on T5 to T7.
                Furthermore TD+H-CNN achieves result comparable to ours on T4.
                To further evaluate the training efficiency of each model, we provide different amounts of data for
                training.
                We can observe from Figure4 that the scale of training data significantly affects model's performance.
                Unlike our model, humans can learn the task-specific rule from very limited examples.
                Basically all methods would nearly fail if we reduce the amount of training data to 15% of PQA pairs per
                task.
                Compared to other baseline methods however, ours performs the best.
            </p>

            <h3 class="w3-left-align" id="publication"><b>Publication</b></h3>

            <h4 class="w3-left-align" id="github"><b>Code</b></h4>
            <a href="https://github.com/qugank/PQA" target="__blank">GitHub</a>
            |
            <a href="https://drive.google.com/file/d/1FW2SMdd68U2KpSxTG4QEYQfexVn1KjK2/view?usp=sharing"
                target="__blank">trained weight</a>
            |
            <a href="https://drive.google.com/file/d/18g9LXyhcyHfC-2V_BPPQpXk8FztDkXe7/view?usp=sharing"
                target="__blank">init weight</a>

            <h4 class="w3-left-align" id="dataset"><b>Dataset</b></h4>
            <a href="https://drive.google.com/file/d/1OemRMUogRpvQ257de9JA_4NZJwRLyMLb/view?usp=sharing"
                target="__blank">latest version (recommended)</a>
            |
            <a href="https://drive.google.com/file/d/1Ump44358mfhziPMeEpdDYJHEUne9wcLl/view?usp=sharing"
                target="__blank">old version</a>

            <p>
                There are 7 folder in the dataset root folder each represent one of 7 tasks. Each folder contain 20k
                json files.
                All file saved with <a href="https://github.com/fchollet/ARC">ARC</a> data format and can be visualized
                by <a href="https://github.com/fchollet/ARC/tree/master/apps">ARC visualization tool</a>.
            </p>

            <div class="w3-code">
                Each JSON file contains a dictionary with two fields:
                <br>
                "train": demonstration input/output pairs. It is a list of "pairs" (typically 3 pairs).
                <br>
                "test": test input/output pairs. It is a list of "pairs" (typically 1 pair).
                <br>
                <br>
                A "pair" is a dictionary with two fields:
                <br>
                "input": the input "grid" for the pair.
                <br>
                "output": the output "grid" for the pair.
            </div>
            <h4 class="w3-left-align" id="paper"><b>Paper</b></h4>
            <a href="https://github.com/qugank/PQA" target="__blank">Paper</a>

            <div class="w3-code">
                @misc{111111,<br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={???},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={???},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={???},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;eprint={???},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;archivePrefix={???},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </div>
        </div>


    </div>

</body>

</html>