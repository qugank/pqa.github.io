<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>PQA: Perceptual Question Answering</title>
    <link rel="stylesheet" href="w3.css">
</head>

<body>

    <br />
    <br />

    <div class="w3-container">
        <div class="w3-content" style="max-width:1080px">
            <div class="w3-content w3-center" style="max-width:850px">
                <h2 id="title"><b>PQA: Perceptual Question Answering</b></h2>
                <p>
                    <a href="https://qugank.github.io/" target="_blank">Yonggang Qi</a>
                    <sup>1*</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a target="_blank">Kai Zhang</a><sup>1*</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a target="_blank">Aneeshan Sain</a><sup>2</sup>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a target="_blank">Yi-Zhe Song</a><sup>2</sup>
                </p>
                <p>
                    <sup>1</sup>Beijing University of Posts and Telecommunications, CN
                    &nbsp; &nbsp; &nbsp;
                    <sup>2</sup>SketchX, CVSSP, University of Surrey, UK
                </p>
                <p><b>CVPR 2021</b></p>
                <div class="w3-content w3-center" style="max-width:850px">
                    <div style="max-width:850px; display:inline-block">
                    <a href="https://arxiv.org/abs/2104.03589" target="_blank" style="color:#007bff">
                            <img src="PQA.png" alt="front" style="width:50px"/>
                            <div style="margin:10px 0"></div>
                            <b>arXiv</b></a>
                    </div>
                    &emsp;&emsp;&emsp;&emsp;&emsp;
                    <div style="max-width:850px; display:inline-block">
                    <a href="https://drive.google.com/file/d/1Lh06RY0UxJDW5t3YWZsLqxyK-tLX3iKX/view" target="_blank" style="color:#007bff">
                            <img src="database.svg" alt="front" style="width:50px"/>
                            <div style="margin:10px 0"></div>
                            <b>Dataset</b></a>
                    </div>
                    &emsp;&emsp;&emsp;&emsp;&emsp;
                    <div style="max-width:850px; display:inline-block">
                    <a href="https://github.com/qugank/PQA" target="_blank" style="color:#007bff">
                            <img src="github.png" alt="front" style="width:50px"/>
                            <div style="margin:10px 0"></div>
                            <b>Code</b></a>
                    </div>
                </div>
            </div>

            <br>
            <div class="w3-content w3-center" style="max-width:850px">
                <img src="front-v2.svg" alt="front" style="width:580px"/>
                <p>Figure 1. Perceptual Question and Answer (PQA). Given an ex-emplar PQA pair (Left), a new question
                    (right) is required to be addressed, i.e. generate answer-grid from scratch.</p>
            </div>
            <br>
            <h3 class="w3-left-align" id="introduction"><b>Introduction</b></h3>
            <p>
                Perceptual organization remains one of the very few established theories on the human visual system. 
                It underpinned many pre-deep seminal works on segmentation and detection, yet research has seen a rapid decline since the preferential shift to learning deep models. 
                Of the limited attempts, most aimed at interpreting complex visual scenes using perceptual organizational rules. This has however been proven to be sub-optimal, since models were unable to effectively capture the visual complexity in real-world imagery. 
                In this paper, we rejuvenate the study of perceptual organization, by advocating two positional changes: (i) we examine purposefully generated synthetic data, instead of complex real imagery, and (ii) we ask machines to synthesize novel perceptually-valid patterns, instead of explaining existing data. 
                Our overall answer lies with the introduction of a novel visual challenge -- the challenge of perceptual question answering (PQA). 
                Upon observing example perceptual question-answer pairs, the goal for PQA is to solve similar questions by generating answers entirely from scratch (see Figure 1). 
                Our first contribution is therefore the first dataset of perceptual question-answer pairs, each generated specifically for a particular Gestalt principle. 
                We then borrow insights from human psychology to design an agent that casts perceptual organization as a self-attention problem, where a proposed grid-to-grid mapping network directly generates answer patterns from scratch. 
                Experiments show our agent to outperform a selection of naive and strong baselines. 
            </p>
            <div class="w3-content w3-center" style="max-width:850px">
                <video controls width="640px">
                    <source src="PQA-CVPR21.mp4" type="video/mp4">
                    <p>Your browser doesn't support HTML5 video. Here is
                       a <a href="PQA-CVPR21.mp4">link to the video</a> instead.</p>
                </video>
            </div>

            <h3 class="w3-left-align"><b>PQA Challenge</b></h3>
            <h4 class="w3-left-align"><b>Dataset Preview</b></h4>
            <div class="w3-content w3-center" style="max-width:870px">
                <img src="dataset-v4.svg" alt="dataset" style="width:870px" />
                <p class="w3-left-align">
                    Figure 2. PQA Dataset. Each row from (a) to (g) corresponds to a specific Gestalt law, and a few examples
                    of PQA pair with question (left) and answer (right) are visualized. Zoom in for better visualization.</p>
            </div>

            <h4 class="w3-left-align"><b>Explore More PQA Pairs</b></h4>
            <div class="w3-container">
                <div class="w3-half">
                    <div class="w3-container">
                        <p>
                            We provide visualization of more PQA pairs below. Simply select a task and an index to view.
                        </p>
                        <div class="w3-content w3-center w3-half">
                        <b>Question</b>
                        </div>
                        <div class="w3-content w3-center w3-half">
                        <b>Answer</b>
                        </div>
                        <p class="w3-center">
                        <img id="showQuestion" src="PQA_pair/Closure Filling/1_Q.svg" style="width:230px; height:230px" alt="front" />
                            &emsp;
                        <img id="showAnswer" src="PQA_pair/Closure Filling/1_A.svg" style="width:230px; height:230px" alt="front" />
                        <script>
                            var t; var i;
                            function change(v) {
                                changeTask(); changeImg(v);
                                var task = t; var image = i;
                                document.getElementById("showQuestion").src = "PQA_pair/" + task.value + "/" + image + "_Q.svg";
                                document.getElementById("showAnswer").src = "PQA_pair/" + task.value + "/" + image + "_A.svg";
                            }
                            function changeTask() {
                                t = document.getElementById("task-selector");
                            }
                            function changeImg(v) {
                                if(v>=1 && v<=10)
                                    i = v;
                            }
                        </script>
                        </p>
                    </div>
                </div>

                <div class="w3-half">
                    <div class="w3-container">
                        <h4>Raw Data Format</h4>
<!--                            There are 7 folder in the dataset root folder each represent one of 7 tasks. Each folder contain 20k-->
<!--                            json files.-->
<!--                            All file saved with <a href="https://github.com/fchollet/ARC" target="_blank">ARC</a> data format and can be visualized-->
<!--                            by <a href="https://github.com/fchollet/ARC/tree/master/apps" target="_blank">ARC visualization tool</a>.-->
                        <div class="w3-code">
                            All PQA pairs are stored in JSON file. Each JSON file contains a dictionary with two fields:

                            <br>
                            - "train": a list of exemplar Q/A pairs.

                            <br>
                            - "test": a list of test Q/A pairs.
                            <br>
                            <br>
                            Each "pair" has two fields:
                            <br>
                            - "input": a question "grid".
                            <br>
                            - "output": an output "grid".
                            <br>
                            <br>
                            Each "grid" (width w, height h) is composed of w*h color symbols. Each color symbol is one of 10 pre-defined colors.
                        </div>
                    </div>
                </div>
            </div>
            <div class="w3-container">
                <div class="w3-half">
                    <form class="w3-container w3-center" action="" style="display:inline-block">
                        <select id="task-selector" onchange="change(this.value)" name="Task" style="width:170px; height:36px" >
                            <option selected value="Closure Filling">(a)Closure Filling</option>
                            <option value="Continuity Connection">(b)Continuity Connection</option>
                            <option value="Proximity Identification">(c)Proximity Identification</option>
                            <option value="Shape Reconstruction">(d)Shape Reconstruction</option>
                            <option value="Shape&Pattern Similarity">(e)Shape&Pattern Similarity</option>
                            <option value="Reflection Symmetry">(f)Reflection Symmetry</option>
                            <option value="Rotation Symmetry">(g)Rotation Symmetry</option>
                        </select>
                    </form>

                    <div class="w3-bar w3-round" id="image-selector" style="height:36px; display:inline-block">
                        <button class="w3-bar-item w3-hover-blue" value="1" onclick="change(this.value)">1</button >
                        <button class="w3-bar-item w3-hover-blue" value="2" onclick="change(this.value)">2</button >
                        <button class="w3-bar-item w3-hover-blue" value="3" onclick="change(this.value)">3</button >
                        <button class="w3-bar-item w3-hover-blue" value="4" onclick="change(this.value)">4</button >
                        <button class="w3-bar-item w3-hover-blue" value="5" onclick="change(this.value)">5</button >
                        <button class="w3-bar-item w3-hover-blue" value="6" onclick="change(this.value)">6</button >
                        <button class="w3-bar-item w3-hover-blue" value="7" onclick="change(this.value)">7</button >
                        <button class="w3-bar-item w3-hover-blue" value="8" onclick="change(this.value)">8</button >
                        <button class="w3-bar-item w3-hover-blue" value="9" onclick="change(this.value)">9</button >
                        <button class="w3-bar-item w3-hover-blue" value="10" onclick="change(this.value)">10</button >
                    </div>
                </div>

                <div class="w3-half">
                    <div class="w3-container">
                        <a href="template.json" target="_blank">
                            <button class="w3-btn w3-white w3-border w3-border-blue w3-hover-blue w3-round-large">An Example of JSON file</button></a>
                    </div>
                </div>
            </div>


<!--            <h4 class="w3-left-align"><b>Statistic</b></h4>-->
<!--            <div class="w3-cell-row">-->

<!--                <div class="w3-container w3-cell w3-cell-middle">-->
<!--                    <div class="w3-content w3-center">-->
<!--                        <img src="statistic.svg" alt="statistic"  style="width:80%"/>-->
<!--                        <p>-->
<!--                            Figure 3.-->
<!--                            (a) Distribution of key region loca-tions. -->
<!--                            (b) Distribution of grid size.-->
<!--                        </p>-->
<!--                    </div>-->
<!--                </div>-->
<!--              -->
<!--                <div class="w3-container w3-cell w3-cell-middle">-->
<!--                    <div class="w3-content w3-center" style="width:120%" >-->
<!--                        <table class="w3-table w3-bordered w3-border">-->
<!--                            <tr>-->
<!--                                <th>Tasks </th>-->
<!--                                <th>T<sup>1</sup></th>-->
<!--                                <th>T<sup>2</sup></th>-->
<!--                                <th>T<sup>3</sup></th>-->
<!--                                <th>T<sup>4</sup></th>-->
<!--                                <th>T<sup>5</sup></th>-->
<!--                                <th>T<sup>6</sup></th>-->
<!--                                <th>T<sup>7</sup></th>-->
<!--                            </tr>-->
<!--                            <tr>-->
<!--                                <td>Avg Symbols</td>-->
<!--                                <td>2.0</td>-->
<!--                                <td>2.0</td>-->
<!--                                <td>5.0</td>-->
<!--                                <td>2.0</td>-->
<!--                                <td>5.0</td>-->
<!--                                <td>3.0</td>-->
<!--                                <td>5.0</td>-->
<!--                            </tr>-->
<!--                            <tr>-->
<!--                                <td>Avg Slots (%)</td>-->
<!--                                <td>12.9</td>-->
<!--                                <td>3.6</td>-->
<!--                                <td>4.0</td>-->
<!--                                <td>7.6</td>-->
<!--                                <td>15.3</td>-->
<!--                                <td>9.8</td>-->
<!--                                <td>12.5</td>-->
<!--                            </tr>-->
<!--                        </table>-->
<!--                        <p> Table 1. Statistics of PQA dataset. </p>-->
<!--                    </div>-->
<!--                </div>-->
<!--              -->
<!--            </div>-->

<!--            <p>-->
<!--                Statistical analysis is provided as shown in Figure 3 and Table 1 above where Avg Symbols indicate the-->
<!--                number of symbols in a question-grid.-->
<!--                The x and y coordinates in Figure 3 (a) are normalized to (0,1), corresponding to the center of key regions, -->
<!--                x-axis and y-axis in Figure 3 (b) correspond to width and height of a grid.-->
<!--                It basically shows the number of colors that are enough to represent a specified instance of a task.-->
<!--                For instance, 2 colors in T1 are enough &#45;&#45; one for background and one for the boundary of closure-->
<!--                region.-->
<!--                Avg Slots represents the percentage of question-grid needed to be modified to form a correct answer.-->
<!--            </p>-->

            <h3 class="w3-left-align"><b>Our Solution</b></h3>
            <div class="w3-content w3-center" style="max-width:1000px">
                <img src="network-v2.svg" alt="network" style="width:900px" />
                <p>
                    Figure 3. Framework overview.
<!--                    The encoder takes test question embedding, positional encoding and context embedding as inputs,-->
<!--                     where context embedding is given by a context encoder, providing clues about the implied law in an example PQA pair, and the positional encoding adapts to the 2D case. -->
<!--                    The decoder can generate an answer-grid by predicting all symbols at every location in parallel.-->
                </p>
            </div>

            <p>
                As shown in Figure 3, our proposed network is an encoder-decoder architecture which is tailored from Transformer.
                In general, the encoder is a stack of N identical layers, and each layer takes inputs from three sources: 
                (i) test question embedding (the first layer) or output of last layer (the other layers), (ii) positional encoding and (iii) context embedding.
                The decoder then generates an answer-grid by predicting all symbols at every location of the grid.

                There are three fundamental components in our transformer architecture: 
                (i) A context embedding module based on self-attention mechanism that is used to encode exemplar PA grid pairs which itself is a part of the input fed into the input encoder. 
                This plays a significant role in inferring the implicit Gestalt law and later generalizing onto the new test question. 
                (ii) We extend positional encoding to adapt to 2D grids-case instead of working on 1D sequence-case, as the 2D locations of symbols are of crucial importance for our problem. 
                (iii) As ours is a 2D grid-to-grid mapping problem, the decoder is tailored to predict all symbols in parallel, i.e., 
                all the color grids are produced in one pass to form an answer instead of one output at a time. 
            </p>

            <h3 class="w3-left-align" id="results"><b>Results</b></h3>

            <p>
                For each question, one credit is given only if all the symbols of the generated answer are correct, i.e., error-free criteria, no credit at all otherwise. The accuracy is thus the percentage of absolute correct answers. Results are shown in Table 1.
            </p>
            <div class="w3-content w3-center" style="max-width:650px">
                <table class="w3-table w3-bordered w3-border">
                    <tr>
                        <th>Method</th>
                        <th>T1</th>
                        <th>T2</th>
                        <th>T3</th>
                        <th>T4</th>
                        <th>T5</th>
                        <th>T6</th>
                        <th>T7</th>
                        <th>Avg </th>
                    </tr>
                    <tr>
                        <td>ResNet-34</td>
                        <td>79.6</td>
                        <td>17.6</td>
                        <td>17.9</td>
                        <td>85.2</td>
                        <td>0</td>
                        <td>19.6</td>
                        <td>0.1</td>
                        <td>31.4 </td>
                    </tr>
                    <tr>


                        <td>ResNet-101</td>
                        <td>73.9</td>
                        <td>10.6</td>
                        <td>0.1</td>
                        <td>50.9</td>
                        <td>0</td>
                        <td>1.7</td>
                        <td>0</td>
                        <td>19.6 </td>
                    </tr>
                    <tr>
                        <td>LSTM</td>
                        <td>55.7</td>
                        <td>23.2</td>
                        <td>25.6</td>
                        <td>38.2</td>
                        <td>0</td>
                        <td>7.4</td>
                        <td>2.8</td>
                        <td>21.8</td>
                    </tr>
                    <tr>
                        <td>bi-LSTM</td>
                        <td>81.9</td>
                        <td>26.6</td>
                        <td>75.6</td>
                        <td>85.9</td>
                        <td>0</td>
                        <td>41.4</td>
                        <td>23.4</td>
                        <td>47.8</td>
                    </tr>
                    <tr>
                        <td>Transformer</td>
                        <td>16.8</td>
                        <td>11.3</td>
                        <td>87.4</td>
                        <td>0.3</td>
                        <td>0</td>
                        <td>0.1</td>
                        <td>0</td>
                        <td>16.7 </td>
                    </tr>
                    <tr>
                        <td>TD+H-CNN</td>
                        <td> <b> 88.8 </b></td>
                        <td>89.8</td>
                        <td>78.8</td>
                        <td>96.4</td>
                        <td>0</td>
                        <td>50.8</td>
                        <td>9.3</td>
                        <td>59.1</td>
                    </tr>
                    <tr>
                        <td>Ours</td>
                        <td>82.6</td>
                        <td><b>97.6</b></td>
                        <td><b>93.7</b></td>
                        <td><b>96.9</b></td>
                        <td><b>61.8</b></td>
                        <td><b>82.7</b></td>
                        <td><b>98.9</b></td>
                        <td><b>87.8</b></td>
                    </tr>
                </table>
                <p>Table 1. Comparison results (%) of models trained on all tasks.</p>
            </div>

            <p>To further evaluate the training efficiency of each model, we provide different amounts of data for training.
                We can observe from Figure 4 that the scale of training data significantly affects model's performance.
                Unlike our model, humans can learn the task-specific rule from very limited examples.
                This clearly signifies just how unexplored this topic is, and in turn encourages future research to progress towards human-level intelligence.
            </p>

            <div class="w3-content w3-center" style="max-width:850px">
                <img src="result.svg" alt="result" style="width:600px" />
                <p> Figure 4. Testing results on varying training data volume. </p>
            </div>



<!--            <p>-->
<!--                We observe our method which significantly outperforms other competitors over all tasks.-->
<!--                On inspecting performances of every task further individually in Table2, we realize that T5 is most-->
<!--                challenging as all baseline methods fail completely.-->
<!--                On the contrary, it is interesting to note that humans can understand and address the questions in T5-->
<!--                quite easily.-->
<!--                Similar trend can be found on T6 and T7 as well. On tasks T1 to T4, all competitors perform better than-->
<!--                they do on T5 to T7.-->
<!--                Furthermore TD+H-CNN achieves result comparable to ours on T4.-->
<!--                To further evaluate the training efficiency of each model, we provide different amounts of data for-->
<!--                training.-->
<!--                We can observe from Figure5 that the scale of training data significantly affects model's performance.-->
<!--                Unlike our model, humans can learn the task-specific rule from very limited examples.-->
<!--                Basically all methods would nearly fail if we reduce the amount of training data to 15% of PQA pairs per-->
<!--                task.-->
<!--                Compared to other baseline methods however, ours performs the best.-->
<!--            </p>-->

<!--            <h3 class="w3-left-align" id="publication"><b>Publication</b></h3>-->

<!--            <h4 class="w3-left-align" id="github"><b>Code</b></h4>-->
<!--            <a href="https://github.com/qugank/PQA/code" target="__blank">GitHub</a>-->
<!--            |-->
<!--            <a href="https://drive.google.com/file/d/1FW2SMdd68U2KpSxTG4QEYQfexVn1KjK2/view?usp=sharing"-->
<!--                target="__blank">trained weight</a>-->

<!--            <h4 class="w3-left-align" id="dataset"><b>Dataset</b></h4>-->
<!--            <a href="https://drive.google.com/file/d/1GOIiqUuRy1SCXMDoDRPB_sxRuTmd8XYS/view?usp=sharing"-->
<!--                target="__blank">download</a>-->

<!--            <p>-->
<!--                There are 7 folder in the dataset root folder each represent one of 7 tasks. Each folder contain 20k-->
<!--                json files.-->
<!--                All file saved with <a href="https://github.com/fchollet/ARC">ARC</a> data format and can be visualized-->
<!--                by <a href="https://github.com/fchollet/ARC/tree/master/apps">ARC visualization tool</a>.-->
<!--            </p>-->

<!--            <div class="w3-code">-->
<!--                Each JSON file contains a dictionary with two fields:-->
<!--                <br>-->
<!--                "train": demonstration input/output pairs. It is a list of "pairs" (typically 3 pairs).-->
<!--                <br>-->
<!--                "test": test input/output pairs. It is a list of "pairs" (typically 1 pair).-->
<!--                <br>-->
<!--                <br>-->
<!--                A "pair" is a dictionary with two fields:-->
<!--                <br>-->
<!--                "input": the input "grid" for the pair.-->
<!--                <br>-->
<!--                "output": the output "grid" for the pair.-->
<!--            </div>-->
            <h4 class="w3-left-align" id="Bib"><b>Bibtex</b></h4>
            
            If this <a href="https://github.com/qugank/PQA" target="__blank">work</a> is useful for you, please cite it:
            <div class="w3-code">
                @inproceedings{yonggang2021pqa,<br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={PQA: Perceptual Question Answering},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Yonggang Qi, Kai Zhang, Aneeshan Sain, Yi-Zhe Song},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={CVPR},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2021}<br>
                }
            </div>
        </div>

        <hr/>  
        <div class="w3-content w3-center w3-opacity" style="max-width:850px"> <p style="font-size: xx-small;color: grey;">Proudly created by Kai Zhang @ BUPT <br> 2021.3 </p> </div>

    </div>

</body>

</html>
